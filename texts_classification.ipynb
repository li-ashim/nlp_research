{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Classifying news based on their content</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<ol><b>\r\n",
    "    <li>Vectorizing text data</li>\r\n",
    "    <li>Dimensiality reduction</li>\r\n",
    "    <li>Training classifiers</li>\r\n",
    "</b></ol> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TransformerMixin\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\r\n",
    "import razdel\r\n",
    "import pymorphy2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "np.random.seed(42)\r\n",
    "pd.set_option('max_colwidth', 120)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Data</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('fakenews_dataset.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['fake'].value_counts(), '\\n')\r\n",
    "print(df['fake'].value_counts(normalize=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Fake')\r\n",
    "print((df[df['fake'] == 1]['content'].apply(len)).agg([np.min, np.max, np.mean, np.median]), '\\n')\r\n",
    "\r\n",
    "print('Not fake')\r\n",
    "print((df[df['fake'] == 0]['content'].apply(len)).agg([np.min, np.max, np.mean, np.median]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Vectorization</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "morph = pymorphy2.MorphAnalyzer(result_type=None)\r\n",
    "def lemmatize(word):\r\n",
    "    return morph.normal_forms(word)[0]\r\n",
    "\r\n",
    "def tokenize(text):\r\n",
    "    return [lemmatize(token.text) for token in razdel.tokenize(text)]\r\n",
    "        \r\n",
    "# class DenseTransformer(TransformerMixin):\r\n",
    "#     def fit(self, X, y=None, **fit_params):\r\n",
    "#         return self\r\n",
    "\r\n",
    "#     def transform(self, X, y=None, **fit_params):\r\n",
    "#         return X.todense()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=0.0005, max_df=1.0)  # with min_df = 0.001: 9647 terms;;; 0.0005: 15256;;; 1: 59767\r\n",
    "tfidf_docs = vectorizer.fit_transform(df['content'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(vectorizer.vocabulary_) + len(vectorizer.stop_words_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tfidf_docs.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Dimensiality reduction (LSA)</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_MSE(reconstructed_tfidf, tfidf_docs):\r\n",
    "    return np.sqrt(((tfidf_docs.toarray() - reconstructed_tfidf).flatten()**2).mean()).round(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "errors = []\r\n",
    "\r\n",
    "params = [1, 2, 5, 10, 20, 50, 100, 150, 200]\r\n",
    "\r\n",
    "for param in params:\r\n",
    "    lsa = TruncatedSVD(n_components=param, n_iter=5, random_state=42)\r\n",
    "    dtm = lsa.fit_transform(tfidf_docs)\r\n",
    "    reconstructed_tfidf = lsa.inverse_transform(dtm)\r\n",
    "    error = get_MSE(reconstructed_tfidf, tfidf_docs)\r\n",
    "    errors.append(error)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "plt.ylabel('MSE')\r\n",
    "plt.xlabel('n_components')\r\n",
    "plt.scatter(params, errors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lsa = TruncatedSVD(n_components=100, n_iter=5, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Classification</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Baseline Model</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Baseline model - Zero rule classifier\r\n",
    "# has accuracy 0.91\r\n",
    "# but 0 precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dtm, df['fake'], \r\n",
    "                                                    test_size=0.4, random_state=42)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11360/1152628429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(dtm, df['fake'], \n\u001b[0m\u001b[0;32m      2\u001b[0m                                                     test_size=0.4, random_state=42)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Logistic regression</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr_clf = LogisticRegression(class_weight={1: 10})\r\n",
    "lr_clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr_clf.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_lr = lr_clf.predict(X_test)\r\n",
    "prediction_lr[prediction_lr < 0] = 0\r\n",
    "prediction_lr[prediction_lr > 0] = 1\r\n",
    "\r\n",
    "print(f'Recall: {recall_score(y_test, prediction_lr)}')\r\n",
    "print(f'F1: {f1_score(y_test, prediction_lr)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>SVC</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.svm import SVC"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc_clf = SVC(kernel='linear', random_state=42, class_weight={1: 10})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc_clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc_clf.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_svc = svc_clf.predict(X_test)\r\n",
    "\r\n",
    "print(f'Recall: {recall_score(y_test, prediction_svc)}')\r\n",
    "print(f'F1: {f1_score(y_test, prediction_svc)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Random Forest</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, class_weight={1: 10})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_rf = rf.predict(X_test)\r\n",
    "print(f'Recall: {recall_score(y_test, prediction_rf)}')\r\n",
    "print(f'F1: {f1_score(y_test, prediction_rf)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Pipelines</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.pipeline import Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['fake'], test_size=0.4, random_state=42)\r\n",
    "X_full = np.hstack((X_train, X_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "class TrainTestTransformer(TransformerMixin):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.train = True\r\n",
    "\r\n",
    "    def fit(self, X, y=None, rs=42, train=True, **fit_params):\r\n",
    "        if train:\r\n",
    "            self.train = True\r\n",
    "        self.X_train, self.X_test = train_test_split(X, test_size=0.4, shuffle=False, random_state=rs)\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X, y=None, **fit_params):\r\n",
    "        if self.train:\r\n",
    "            self.train = False\r\n",
    "            return self.X_train\r\n",
    "        else:\r\n",
    "            return self.X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "estimators = [('vectorize', TfidfVectorizer(tokenizer=tokenize, min_df=1, max_df=1.0)), \r\n",
    "              ('reduce_dim', TruncatedSVD(n_components=25, random_state=42)),\r\n",
    "              ('train_test_split', TrainTestTransformer()), \r\n",
    "              ('clf', SVC(kernel='linear', class_weight={1: 10}))]\r\n",
    "\r\n",
    "pipe = Pipeline(estimators, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "pipe.fit(X_full, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Pipeline] ......... (step 1 of 4) Processing vectorize, total= 8.4min\n",
      "[Pipeline] ........ (step 2 of 4) Processing reduce_dim, total=   1.9s\n",
      "[Pipeline] .. (step 3 of 4) Processing train_test_split, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.1s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorize',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x0000026FD5C91DC8>)),\n",
       "                ('reduce_dim', TruncatedSVD(n_components=25, random_state=42)),\n",
       "                ('train_test_split',\n",
       "                 <__main__.TrainTestTransformer object at 0x0000026FDC308808>),\n",
       "                ('clf', SVC(class_weight={1: 10}))],\n",
       "         verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "pred = pipe.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, pred)}')\r\n",
    "print(f'Recall: {recall_score(y_test, pred)}')\r\n",
    "print(f'F1: {f1_score(y_test, pred)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.9975476839237057\n",
      "Recall: 0.9753424657534246\n",
      "F1: 0.9875173370319001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Model selection with Grid Search</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "param_grid = dict(reduce_dim__n_components=[25, 50, 100, 200],\r\n",
    "                  clf=[LogisticRegression(class_weight={1: 10}), SVC(kernel='rbf', class_weight={1: 10}), \r\n",
    "                       SVC(kernel='linear', class_weight={1: 10}), \r\n",
    "                       SVC(kernel='sigmoid', class_weight={1: 10}), \r\n",
    "                       RandomForestClassifier(n_estimators=100, class_weight={1: 10}), \r\n",
    "                       RandomForestClassifier(n_estimators=150, class_weight={1: 10}), \r\n",
    "                       RandomForestClassifier(n_estimators=50, class_weight={1: 10})])\r\n",
    "\r\n",
    "estimators = [('reduce_dim', TruncatedSVD(random_state=42)),\r\n",
    "              ('clf', None)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "pipe = Pipeline(estimators)\r\n",
    "\r\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=['accuracy', 'recall', 'f1'], \r\n",
    "                           refit='recall', verbose=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for min_df in [0.0001, 0.0005, 0.001, 0.002]:\r\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=min_df, max_df=1.0)\r\n",
    "    tfidf_docs = vectorizer.fit_transform(df['content'])\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tfidf_docs, df['fake'], test_size=0.4, random_state=42)\r\n",
    "    grid_search.fit(X_train, y_train)\r\n",
    "    print(f'\\n\\nVectorization with min_df = {min_df}')\r\n",
    "    print(grid_search.best_estimator_.get_params())\r\n",
    "    pred = grid_search.best_estimator_.predict(X_test)\r\n",
    "\r\n",
    "    print(f'Accuracy: {accuracy_score(y_test, pred)}')\r\n",
    "    print(f'Recall: {recall_score(y_test, pred)}')\r\n",
    "    print(f'F1: {f1_score(y_test, pred)}')\r\n",
    "    print('-'*100, '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Vectorization with min_df = 0.0001\n",
      "{'memory': None, 'steps': [('reduce_dim', TruncatedSVD(n_components=25, random_state=42)), ('clf', SVC(class_weight={1: 10}))], 'verbose': False, 'reduce_dim': TruncatedSVD(n_components=25, random_state=42), 'clf': SVC(class_weight={1: 10}), 'reduce_dim__algorithm': 'randomized', 'reduce_dim__n_components': 25, 'reduce_dim__n_iter': 5, 'reduce_dim__random_state': 42, 'reduce_dim__tol': 0.0, 'clf__C': 1.0, 'clf__break_ties': False, 'clf__cache_size': 200, 'clf__class_weight': {1: 10}, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'clf__max_iter': -1, 'clf__probability': False, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}\n",
      "Accuracy: 0.9978201634877384\n",
      "Recall: 0.9780821917808219\n",
      "F1: 0.9889196675900276\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "Vectorization with min_df = 0.0005\n",
      "{'memory': None, 'steps': [('reduce_dim', TruncatedSVD(n_components=25, random_state=42)), ('clf', LogisticRegression(class_weight={1: 10}))], 'verbose': False, 'reduce_dim': TruncatedSVD(n_components=25, random_state=42), 'clf': LogisticRegression(class_weight={1: 10}), 'reduce_dim__algorithm': 'randomized', 'reduce_dim__n_components': 25, 'reduce_dim__n_iter': 5, 'reduce_dim__random_state': 42, 'reduce_dim__tol': 0.0, 'clf__C': 1.0, 'clf__class_weight': {1: 10}, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__l1_ratio': None, 'clf__max_iter': 100, 'clf__multi_class': 'auto', 'clf__n_jobs': None, 'clf__penalty': 'l2', 'clf__random_state': None, 'clf__solver': 'lbfgs', 'clf__tol': 0.0001, 'clf__verbose': 0, 'clf__warm_start': False}\n",
      "Accuracy: 0.9967302452316076\n",
      "Recall: 0.9671232876712329\n",
      "F1: 0.9832869080779943\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "Vectorization with min_df = 0.001\n",
      "{'memory': None, 'steps': [('reduce_dim', TruncatedSVD(n_components=25, random_state=42)), ('clf', LogisticRegression(class_weight={1: 10}))], 'verbose': False, 'reduce_dim': TruncatedSVD(n_components=25, random_state=42), 'clf': LogisticRegression(class_weight={1: 10}), 'reduce_dim__algorithm': 'randomized', 'reduce_dim__n_components': 25, 'reduce_dim__n_iter': 5, 'reduce_dim__random_state': 42, 'reduce_dim__tol': 0.0, 'clf__C': 1.0, 'clf__class_weight': {1: 10}, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__l1_ratio': None, 'clf__max_iter': 100, 'clf__multi_class': 'auto', 'clf__n_jobs': None, 'clf__penalty': 'l2', 'clf__random_state': None, 'clf__solver': 'lbfgs', 'clf__tol': 0.0001, 'clf__verbose': 0, 'clf__warm_start': False}\n",
      "Accuracy: 0.9967302452316076\n",
      "Recall: 0.9671232876712329\n",
      "F1: 0.9832869080779943\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "Vectorization with min_df = 0.002\n",
      "{'memory': None, 'steps': [('reduce_dim', TruncatedSVD(n_components=25, random_state=42)), ('clf', LogisticRegression(class_weight={1: 10}))], 'verbose': False, 'reduce_dim': TruncatedSVD(n_components=25, random_state=42), 'clf': LogisticRegression(class_weight={1: 10}), 'reduce_dim__algorithm': 'randomized', 'reduce_dim__n_components': 25, 'reduce_dim__n_iter': 5, 'reduce_dim__random_state': 42, 'reduce_dim__tol': 0.0, 'clf__C': 1.0, 'clf__class_weight': {1: 10}, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__l1_ratio': None, 'clf__max_iter': 100, 'clf__multi_class': 'auto', 'clf__n_jobs': None, 'clf__penalty': 'l2', 'clf__random_state': None, 'clf__solver': 'lbfgs', 'clf__tol': 0.0001, 'clf__verbose': 0, 'clf__warm_start': False}\n",
      "Accuracy: 0.9967302452316076\n",
      "Recall: 0.9671232876712329\n",
      "F1: 0.9832869080779943\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorization with min_df = 0.0001\r\n",
    "{'memory': None, 'steps': [('reduce_dim', TruncatedSVD(n_components=25, random_state=42)), ('clf', SVC(class_weight={1: 10}))], 'verbose': False, 'reduce_dim': TruncatedSVD(n_components=25, random_state=42), 'clf': SVC(class_weight={1: 10}), 'reduce_dim__algorithm': 'randomized', 'reduce_dim__n_components': 25, 'reduce_dim__n_iter': 5, 'reduce_dim__random_state': 42, 'reduce_dim__tol': 0.0, 'clf__C': 1.0, 'clf__break_ties': False, 'clf__cache_size': 200, 'clf__class_weight': {1: 10}, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'clf__max_iter': -1, 'clf__probability': False, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}\r\n",
    "Accuracy: 0.9978201634877384\r\n",
    "Recall: 0.9780821917808219\r\n",
    "F1: 0.9889196675900276"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('nlp')"
  },
  "interpreter": {
   "hash": "a8c840506e94c7ebd4e9fbc2369aa3943e31f3b1278726fbb23604191abd28fd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}